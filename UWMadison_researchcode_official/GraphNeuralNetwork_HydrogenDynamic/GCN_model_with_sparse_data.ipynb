{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This create data size appropriate to run keras model. The result turns out to be 80% acuracy, which is not too bad!\n",
    "#running the model through more data would improve accuracy, but will take a lot of time.\n",
    "#code model run for 20 minutes through each itinery on a normal intel core 5 8 RAM latop. thousands of data would require a super computer to be fast.\n",
    "#stronger processor is recommended.\n",
    "\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import spektral\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "487e9bde",
   "metadata": {},
   "source": [
    "load all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data made on organize_data_for_gnn into here\n",
    "with open(r\"C:\\Users\\buing\\Documents\\school research things\\New research (machine learning)\\wc_hydroxy-undecanethiol_k1.0.pickle\", 'rb') as f:\n",
    "    node = pickle.load(f)\n",
    "    adj = pickle.load(f)\n",
    "    y = pickle.load(f)\n",
    "\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b8d7bcb",
   "metadata": {},
   "source": [
    "train test split \n",
    "create training size and testing size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05c366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  1,  8, 18])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#make train/test list\n",
    "n = np.arange(int(len(y) / 200))\n",
    "train_ind, test_ind = train_test_split(n, random_state=0, test_size=0.2)\n",
    "test_ind\n",
    "#make 80% train size and 20% test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e3994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty array of node/adj/energy value results.\n",
    "node_train, adj_train, y_train = [], [], []\n",
    "node_test, adj_test, y_test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df3e7aeb-2d5d-4a5b-9b70-d7b4d509cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#push interval of node/adj/hydrogen value with size 200 for testing batch.\n",
    "for ind in train_ind:\n",
    "    node_train.append(node[ind * 200 : (ind + 1) * 200])\n",
    "    adj_train.append(adj[ind * 200 : (ind + 1) * 200])\n",
    "    y_train.append(y[ind * 200 : (ind + 1) * 200])\n",
    "    \n",
    "for ind in test_ind:\n",
    "    node_test.append(node[ind * 200 : (ind + 1) * 200])\n",
    "    adj_test.append(adj[ind * 200 : (ind + 1) * 200])\n",
    "    y_test.append(y[ind * 200 : (ind + 1) * 200])\n",
    "    \n",
    "import itertools\n",
    "node_train = list(itertools.chain(*node_train))\n",
    "adj_train = list(itertools.chain(*adj_train))\n",
    "y_train = list(itertools.chain(*y_train))\n",
    "node_test = list(itertools.chain(*node_test))\n",
    "adj_test = list(itertools.chain(*adj_test))\n",
    "y_test = list(itertools.chain(*y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d635468-8855-460a-be04-a06d74086b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b9acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_train = np.random.RandomState(0).permutation(node_train)\n",
    "adj_train = np.random.RandomState(0).permutation(adj_train)\n",
    "y_train = np.random.RandomState(0).permutation(y_train)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cae938df",
   "metadata": {},
   "source": [
    "we have 3000 graphs.\n",
    "these graphs are in sparse representation, we can't convert them all back to full representation.\n",
    "so we need to build custom data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30325679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # Generate data for keras\n",
    "    def __init__(self, data, batch_size=15, shuffle=True, validation =True, test = False, demonstration = False ): #validation also = train\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.demonstration = demonstration\n",
    "        if demonstration:\n",
    "            self.list_IDs_all = np.arange(100)\n",
    "        else:\n",
    "            self.list_IDs_all = np.arange(len(data[0]))\n",
    "        self.validation = validation # = self.train\n",
    "        self.test = test\n",
    "        if test:\n",
    "            self.list_IDs = self.list_IDs_all\n",
    "        else:\n",
    "            if self.validation:\n",
    "                _, self.list_IDs = train_test_split(self.list_IDs_all, random_state=0, test_size=0.2)\n",
    "            else:\n",
    "                self.list_IDs, _ = train_test_split(self.list_IDs_all, random_state=0, test_size=0.2)\n",
    "        self.indices = np.arange(len(self.list_IDs))\n",
    "        \n",
    "        #self.train = train\n",
    "        #if self.train:\n",
    "        #    self.list_IDs, _ = train_test_split(self.list_IDs_all, random_state=0, test_size=0.2)\n",
    "        #else:\n",
    "        #    _, self.list_IDs = train_test_split(self.list_IDs_all, random_state=0, test_size=0.2)\n",
    "        #self.indices = np.arange(len(self.list_IDs))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # denote the number of batches per epoch\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # generate one batch of data\n",
    "        ind = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[i] for i in ind]\n",
    "        \n",
    "        # generate data\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # updates indices after each epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # generate data containing batch_size samples\n",
    "        node = []\n",
    "        adj = []\n",
    "        y = []\n",
    "        \n",
    "        for i in list_IDs_temp:\n",
    "            node_temp = self.data[0][i].toarray() # convert sparse array back to dense array\n",
    "            adj_temp = self.data[1][i].toarray()\n",
    "            node.append(node_temp)\n",
    "            adj.append(adj_temp)\n",
    "            y.append(self.data[2][i])\n",
    "        node = np.array(node)\n",
    "        adj = np.array(adj)\n",
    "        adj = spektral.utils.convolution.gcn_filter(adj)\n",
    "        y = np.array(y)\n",
    "        return [node, adj], y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe036574",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "train_generator = DataGenerator(data=[node_train, adj_train, y_train], \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True, \n",
    "                                    validation=False, \n",
    "                                    test=False, \n",
    "                                    demonstration=False)\n",
    "    \n",
    "valid_generator = DataGenerator(data=[node_train, adj_train, y_train], \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True, \n",
    "                                    validation=True, \n",
    "                                    test=False,\n",
    "                                    demonstration=False)\n",
    "    \n",
    "test_generator = DataGenerator(data=[node_train, adj_train, y_train], \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=True, \n",
    "                                   validation=None, \n",
    "                                   test=True,\n",
    "                                   demonstration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bf2677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 2280, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[0][0][0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03bc289d",
   "metadata": {},
   "source": [
    "the generator looks good"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53d6cdf0",
   "metadata": {},
   "source": [
    "define the gcn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0939071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    x_node = layers.Input(shape=(2280, 7))\n",
    "    x_adj = layers.Input(shape=(2280, 2280))\n",
    "    #hyper-parameter 1\n",
    "    # Tune the number of channels in the GCN layers\n",
    "    # Choose an optimal value between 8-64\n",
    "    hp_units_1 = hp.Int('channels', min_value=8, max_value=64, step=8)\n",
    "    #change channels = value seen, since it will take forever.\n",
    "    x = spektral.layers.GCNConv(channels=hp_units_1, activation='relu')([x_node, x_adj])\n",
    "    x = spektral.layers.GCNConv(channels=hp_units_1, activation='relu')([x, x_adj])\n",
    "    \n",
    "    #hyper-parameter 2\n",
    "    # Tune the number of channels in the Global Attention Pool\n",
    "    # Choose an optimal value between 8-64\n",
    "    hp_units_2 = hp.Int('channels', min_value=8, max_value=64, step=8)\n",
    "    x = spektral.layers.GlobalAttentionPool(channels = hp_units_2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    #hyper-parameter 3\n",
    "    # Tune the number of units in the Dense layers\n",
    "    # Choose an optimal value between 8-64\n",
    "    hp_units_3 = hp.Int('units', min_value=8, max_value=64, step=8)\n",
    "    \n",
    "    x = layers.Dense(units=hp_units_3, activation='relu')(x)\n",
    "    x = layers.Dense(units=hp_units_3, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[x_node, x_adj], outputs=x)\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate= hp_learning_rate), \n",
    "                                                  loss='mse', metrics=['mae'])\n",
    "    #model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    #filename = r\"\\GCN_checkpointrun.sav\"\n",
    "    #pickle.dump(model, open(filename, 'wb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ad2c54c",
   "metadata": {},
   "source": [
    "compile the model - set check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2251d1fb-b2ac-4521-a3df-ae9997ac8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model to run\n",
    "x_node = layers.Input(shape=(2280, 7))\n",
    "x_adj = layers.Input(shape=(2280, 2280))\n",
    "    #hyper-parameter 1\n",
    "    # Tune the number of channels in the GCN layers\n",
    "    # Choose an optimal value between 8-64\n",
    "    #change channels = value seen, since it will take forever.\n",
    "x = spektral.layers.GCNConv(channels=8, activation='relu')([x_node, x_adj])\n",
    "x = spektral.layers.GCNConv(channels=8, activation='relu')([x, x_adj])\n",
    "    \n",
    "    #hyper-parameter 2\n",
    "    # Tune the number of channels in the Global Attention Pool\n",
    "    # Choose an optimal value between 8-64\n",
    "x = spektral.layers.GlobalAttentionPool(channels = 8)(x)\n",
    "x = layers.Flatten()(x)\n",
    "    \n",
    "    #hyper-parameter 3\n",
    "    # Tune the number of units in the Dense layers\n",
    "    # Choose an optimal value between 8-64\n",
    "x = layers.Dense(units=16, activation='relu')(x)\n",
    "x = layers.Dense(units=16, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "model = models.Model(inputs=[x_node, x_adj], outputs=x)\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate= 1e-2), \n",
    "                                                loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c77dbe3-687f-4a8c-b73b-665e85f62eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model weights are saved at the end of every epoch, if it's the best seen\n",
    "# so far.\n",
    "#hist = model.fit(training_generator, validation_data=validation_generator, callbacks=[model_checkpoint_callback], epochs=1)\n",
    "#filename = r\"\\GCN_checkpointrun.sav\"\n",
    "#pickle.dump(model, open(filename, 'wb'))\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "#model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ee988-b44a-40bf-89bc-2afa0a849d14",
   "metadata": {},
   "source": [
    "TUNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8244ca3-8a59-45f7-a4da-139e49809ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 06m 36s]\n",
      "val_loss: 42196.41796875\n",
      "\n",
      "Best val_loss So Far: 329.65924072265625\n",
      "Total elapsed time: 00h 15m 16s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "channels          |32                |8                 \n",
      "units             |16                |16                \n",
      "learning_rate     |0.01              |0.01              \n",
      "tuner/epochs      |10                |2                 \n",
      "tuner/initial_e...|4                 |0                 \n",
      "tuner/bracket     |2                 |2                 \n",
      "tuner/round       |2                 |0                 \n",
      "tuner/trial_id    |3d879bb76d04155...|None              \n",
      "\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 73s 13s/step - loss: 213397.1250 - mae: 358.2066 - val_loss: 39378.9648 - val_mae: 195.5854\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 64s 11s/step - loss: 20385.3867 - mae: 125.7165 - val_loss: 12489.6816 - val_mae: 109.3204\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 60s 10s/step - loss: 6623.8682 - mae: 71.3929 - val_loss: 3080.5520 - val_mae: 50.0728\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 58s 10s/step - loss: 3076.2791 - mae: 49.8070 - val_loss: 2797.2026 - val_mae: 48.0795\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 59s 10s/step - loss: 1610.8846 - mae: 35.5167 - val_loss: 667.5011 - val_mae: 19.3542\n",
      "Epoch 10/10\n",
      "4/6 [===================>..........] - ETA: 13s - loss: 1147.4639 - mae: 27.3770"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5896/3113541288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstop_early\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Get the optimal hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tuner/epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"initial_epoch\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tuner/initial_epoch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras_tuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"min\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \"\"\"\n\u001b[0;32m    148\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='./',\n",
    "                     project_name='gnn_hyperparameter')\n",
    "#implement stop early\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_generator, epochs=50, validation_data=valid_generator, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fad9c181-7917-4e24-af76-df1591479c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_hps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5896/1236211072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#loaded_model = model.fit(training_generator, validation_data=validation_generator, callbacks=[model_checkpoint_callback], epochs=4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#current_model = loaded_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbest_hps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_hps' is not defined"
     ]
    }
   ],
   "source": [
    "#pickle.dump(hist.history, open(r\"C:\\Users\\buing\\Documents\\school research things\\New research (machine learning)\\GCN_checkpointrun\", 'wb'))\n",
    "#loaded_model = pickle.load(open(r\"C:\\Users\\buing\\Documents\\school research things\\New research (machine learning)\\GCN_checkpointrun\", 'rb'))\n",
    "#loaded_model = model.fit(training_generator, validation_data=validation_generator, callbacks=[model_checkpoint_callback], epochs=4)\n",
    "#current_model = loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb262cf9-bfd7-40a4-aefe-c077f70fe0f8",
   "metadata": {},
   "source": [
    "set checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4669195-c4a9-494a-9feb-4ec7d5908eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = os.path.join(r\"C:\\Users\\buing\\Documents\\school research things\\New research (machine learning)\\GCN_checkpointrun.sav\")\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath= checkpoint_filepath, monitor='val_loss', mode='min',save_weights_only=False,save_best_only=False)\n",
    "#model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_generator, epochs=50, validation_data=valid_generator, callbacks=[model_checkpoint])\n",
    "\n",
    "#checkpoint_filepath = os.path.join(r\"C:\\Users\\buing\\Documents\\school research things\\New research (machine learning)\\GCN_checkpointrun.sav\")\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath= checkpoint_filepath,\n",
    " #   save_weights_only=True,\n",
    " #   monitor='val_accuracy',\n",
    "  #  mode='max',\n",
    "  #  save_best_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fca1d8-b963-4046-8d42-c74279d57cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41019844-bf79-43ad-8680-11ffcc8346ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
