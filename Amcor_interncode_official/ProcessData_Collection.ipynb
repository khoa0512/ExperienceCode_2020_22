{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c072bf-a85e-4f86-ba80-46c348df37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries to assist coding. In these files, many names/variables/nubmers are changed for confidentiality.\n",
    "#this code takes a break-apart sharepoint link that was stored in excel and combined them into a proper link again.\n",
    "#the code then access the sharepoint site with the pdf tail to access pdf file with NTML security. \n",
    "#The pdf file then get saved, and ghostscript is used to read the pdf table, copy certain info location, into a big dataframe.\n",
    "#The dataframe is then saved as an excel for other team member to analyze.\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import urllib.request as urllib\n",
    "import PyPDF2\n",
    "import io\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "from urllib.parse import urljoin\n",
    "import urllib3\n",
    "import urllib\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests.auth import HTTPDigestAuth\n",
    "import sys\n",
    "from requests_ntlm import HttpNtlmAuth\n",
    "import win32com.client\n",
    "from bs4 import BeautifulSoup\n",
    "import camelot\n",
    "import time\n",
    "import tabula\n",
    "import ghostscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3ec40-5abc-4899-bf9e-49427cbca429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collect = pd.read_excel(r'...xlsx') #excel contains link parts.\n",
    "df1 = df_collect[:][0:700] #links of process over a certain period (ie. 2 mnoths)\n",
    "date = pd.DataFrame(index=range(len(df1[\"Name\"])),columns=range(1)) #create a dataframe of certain size with link date part\n",
    "order_num = pd.DataFrame(index=range(len(df1[\"Name\"])),columns=range(1)) #create another dataframe of certain size with link name part\n",
    "for d in range(len(df1[\"Name\"])):\n",
    "    date.iloc[d] = df1[\"Name\"][d][0:8] #take a part of the date that create the link\n",
    "    order_num.iloc[d] = df1[\"Name\"][d].split()[2] #taje a part of the name that create the name\n",
    "df1[\"Date\"] = date\n",
    "df1[\"Order Number\"] = order_num\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59669a61-d2e6-47ec-b48c-90546903fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9305c-0895-4b8c-b86a-8ecb9718a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the excel empty list for parsing the pdf information in with headerlist A,B,C...Z\n",
    "numRows = len(df1.index)\n",
    "headerlist = [A,B,C,...Z]\n",
    "numCols = len(headerlist) #get length of headerlist\n",
    "df = pd.DataFrame(index=range(numRows),columns=range(numCols)) #get empty dantaframe with specified size with headerlist\n",
    "df.columns= headerlist #make dataframe has heaerlist as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90a84e-baa6-466e-87b6-b4c328787f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(570,len(df1.index)): #manual length size of the date time (ie. 1 month of the 2 month)\n",
    "        url = urljoin(df1[\"Path\"][i],df1[\"Name\"][i]) #join the path and name to make url part\n",
    "        url1 = urllib.parse.quote(url) #parse url\n",
    "        finalurl = urljoin('http://company.net/',url1) #combined together with company website\n",
    "        username = 'username'\n",
    "        password = 'password'\n",
    "        #print(finalurl)\n",
    "        print(i)\n",
    "        sess = requests.Session() #create a session period to access the pdf\n",
    "        sess.auth = HttpNtlmAuth(username,password)\n",
    "        sess.timeout = 10 #give session an open time\n",
    "        r = sess.post(finalurl, stream = True) #access the url.\n",
    "        print(r)\n",
    "        file_path = r\"....pdf\" #create a filepath location on computer to copy pdf over\n",
    "        pdf = open(file_path, 'wb')\n",
    "        pdf.write(r.content) #write content of pdf from site down to file_path pdf\n",
    "        pdf.close()\n",
    "\n",
    "        table = camelot.read_pdf(file_path) #use camelot to read pdf table appropriately\n",
    "        #assign table to variable\n",
    "        df_copy = table[0].df #since pdf when converted on python saved as a matrix of certain size, this code open the matrix where table is content.\n",
    "        #this become a problem if pdf gets \"shifted\", where not all table is located at position 0. However, since there was only about 20 files out of 2000 files that got the error,\n",
    "        #the information could be neglected per team requested, and the format is noted to team for future consistency update and notes.\n",
    "\n",
    "       #copy table values to empty sheets\n",
    "        df.iloc[i][0] = df1[\"A\"][i]\n",
    "        df.iloc[i][1] = df1[\"Date\"][i]\n",
    "        df.iloc[i][2] = df_copy[a][a] \n",
    "        df.iloc[i][3] = ... \n",
    "        df.iloc[i][4] = ... \n",
    "        ...\n",
    "        df.iloc[i][22] = df_copy[b][b].split()[-1] #some data were stored in strings\n",
    "        ..\n",
    "        df.iloc[i][27] = df_copy[c][c].split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "505eb900-adb2-411e-9cfe-1b38bce163a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#double-checking certain paths for diagnostic purpose.\n",
    "url = urljoin(df1[\"Path\"][564],df1[\"Name\"][564])\n",
    "url1 = urllib.parse.quote(url)\n",
    "finalurl = urljoin('...',url1)\n",
    "username = 'usrename'\n",
    "password = 'password'\n",
    "        #print(finalurl)\n",
    "sess = requests.Session()\n",
    "sess.auth = HttpNtlmAuth(username,password)\n",
    "sess.timeout = 10\n",
    "r = sess.post(finalurl, stream = True)\n",
    "print(r)\n",
    "file_path = r\"...\"\n",
    "pdf = open(file_path, 'wb')\n",
    "pdf.write(r.content)\n",
    "pdf.close()\n",
    "table = camelot.read_pdf(file_path)\n",
    "        #assign table to variable\n",
    "df_copy = table[1].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c87e752-c508-4a51-8f07-490d0ec0debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export all copied data to a new excel\n",
    "df.to_excel(r\"....xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707e58e-9079-4a42-9a08-e957dbcf608e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
